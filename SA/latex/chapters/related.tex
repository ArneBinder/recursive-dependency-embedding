\section{Related work}
\todo{UL: rework!! (unklarer Einstieg)} %Since the task of semantic aware composition is very general in nature, there is a broad research landscape on compositional distributional semantics. 
%The survey \textcite{wang_comparison_2017} attempts to identify critical components of composition models. The authors select the dimensions \textit{word representation}, \textit{kind of training data}, \textit{composition function} and \textit{objective function} to classify different kinds of systems. We loosely follow this manner to name important research related to this work.

% binary composition & lingustic motivations
Although not explicitly focus of this work, bi-gram composition forms the basis of compositional semantics since common composition models can be encoded in means of binary composition, at least conceptional\todo{UL:?}. In this manner, \textcite{zanzotto_estimating_2010} names additive and multiplicative based composition as major building blocks. Two individual vector representations are composed by point-wise application of these functions. There are several comparative studies on this topic \autocite{mitchell_composition_2010, dima_reverse-engineering_2015}, inter alia\todo{UL: bad style}, incorporating psycholinguistic information based on analysis of semantic priming effects or compound data. \textcite{mitchell_composition_2010} also concludes, that additive and multiplicative composition models are promising, but performance depends on the source of token embeddings used. % when applied to LDA based embeddings, but multiplicative composition outperforms on co-occurrence based ones. 
%Some research rooted linguistic theory motivated research 
%Further rooted in linguistic theory, 
\textcite{baroni_nouns_2010, clark_type-driven_2013, grefenstette_multi-step_2013, baroni_frege_2014} \todo{UL: methods?} 	distinguish composition by involved word types (i.e. adjectives and nouns) and propose frameworks in which the dimensionality of conceptional token entities or the level of order of the composition functions differs\todo{UL: so what?}. Precisely, by modeling nouns as plain vectors and adjectives as two dimensional matrices, these token entities are intuitively composable in means of modifier application incorporating ideas of Formal Semantics \autocite{montague_proper_1973}, especially semantic composition modeled as function application.\todo{UL Unverständlich. Richtig erklären, oder nicht} %\todo{AB:, but requires explicit distinction of different word types}. %\textcite{pham_sentence_2013} investigates the effect of word order information with respect to \ac{CDSM} performance by looking into transitive sentences. ..	

Semantic relatedness prediction on sentence level was chosen as first task of SemEval 2014\footnote{See \url{http://alt.qcri.org/semeval2014/task1} for further information.}. SemEval is an annual challenge intended to explore the ways natural language represents meaning. Like this work, the SemEval 2014 task utilizes the SICK \autocite{marelli_sick_2014} corpus to evaluate performance of composition models. \textcite{marelli_semeval-2014_2014} summarize the various approaches of submitted models. The majority of participants exploits compositionality features on different degrees of granularity (phrase or full sentence) plus other hand crafted features (e.g., word overlap and similarity, syntactical, alignments, topic modeling). Different learning approaches like Support Vector Machines \autocite{cortes_support-vector_1995} and kernel based methods, Random Forests \autocite{breiman_random_2001} or ensembles \autocite{opitz_popular_1999} were applied. The best performing system \autocite{zhao_ecnu_2014} exploits features related to sentence length, token and dependency type overlap, several distances (e.g., jaccard, cosine, manhatten) applied to \ac{TF-IDF} and latent semantic representations, n-gram data at token and character levels, co-occurrence statistics and summed \ac{LSA} data obtained from other corpora, among others, and applies an ensemble of machine learning methods. The system achieving the third place \autocite{bjerva_meaning_2014} builds on Formal Semantics and logical inference. In addition, it uses summed word2vec \autocite{mikolov_distributed_2013} embeddings to produce sentence representations and calculates the cosine similarity between pairs, we reuse this idea (Section~\ref{subsec:architecture}).\todo{UL: add top-5 scoring systems table (X)}

% CNNs
More recently, approaches that focus on \aclp{ANN} gained increasing attention, leveraged by the rise of semantical rich token embeddings \autocite{mikolov_distributed_2013, pennington_glove_2014, wieting_paraphrase_2015}. The \acf{DAN} \autocite{iyyer_deep_2015} discards any syntactic structure, even ordering information, by averaging all token embeddings of a sentence. Afterwards, a deep \ac{FNN} is applied. This model performs quite well on sentiment classification\todo{UL: Was hat das hier zu suchen?} tasks, comparable to state of the art systems that rely on linear \autocite{kim_convolutional_2014} or hierarchical structure \autocite{tai_improved_2015}. Several other systems \autocite{kim_convolutional_2014,kalchbrenner_convolutional_2014,hu_convolutional_2014,yin_convolutional_2015,he_multi-perspective_2015} exploit \acfp{CNN} \autocite{lecun_convolutional_1995} to compose pretrained token embeddings\todo{UL: Wozu?}. \textcite{he_multi-perspective_2015} uses convolutional filters to incorporate information of different granularities and directions, i.e. along token n-grams or dimensions of token embeddings, to produce sentence embeddings that are feed into a similarity measurement layer which compares selected subsets of sentence embeddings. They use the identical structure and set of weights to embed both sentences, thus their approach follows a \textit{siamese} structure \autocite{bromley_signature_1994}. %Furthermore, \textcite{iyyer_deep_2015} proposes \acp{DAN}. These networks present interesting insights as they perform well on sentiment classif 	

Another line of research uses \ac{RNN} architectures, especially \acfp{LSTM} \autocite{wieting_towards_2015,liu_multi-timescale_2015,mueller_siamese_2016}. As this work, \textcite{wieting_towards_2015} compares averaging of embeddings and \ac{LSTM} based composition. The authors conclude, that simple averaging outperforms the \ac{LSTM} model on the SICK dataset. In contrast to our system, they use embeddings trained on the PPDB \autocite{ganitkevitch_ppdb_2013} dataset that is a very large paraphrase corpus containing pairs of short text spans. \textcite{mueller_siamese_2016} applies a simple \ac{LSTM} in a siamese architecture and uses manhattan metric as similarity measure. In this means, their approach strongly resembles our \ac{LSTM} setting (Section~\ref{subsec:architecture}). Despite its simplicity, their model achieves high performance in the SICK relatedness prediction task. However, the system benefits strongly ($+0.04$ Pearson's $r$) from synonym augmentation that the authors used to double the size of the SICK dataset.
%Though, the system gets major advantage ($+0.04$ Pearson's $r$) by synonym augmentation that the authors used to double the size of the SICK dataset. 
The Skip-Thoughts \autocite{kiros_skip-thought_2015} model follows an encoder-decoder architecture to learn sequence embeddings that perform state of the art at the SICK relatedness task. The system is trained as generative language model, i.e., to produce a meaningful sentence complement given a sentence predecessor. As encoding and decoding units the model uses \acfp{GRU} \autocite{cho_learning_2014}. \acp{GRU} are \acp{RNN} that are less complex then \acp{LSTM} despite performing comparable well. 

Expanding the concept of \acp{RNN} to tree structures, several \acf{RecNN}\footnote{\acp{RecNN} generalize \acp{RNN} in the way that their unfolded computation graph do not have to follow a linear chain, but rather is capable to model an arbitrary tree structure by allowing more than one predecessor.} \autocite{goller_learning_1996} based architectures for semantic aware composition were proposed which rely on pre-calculated parse structure \autocite{socher_dynamic_2011,socher_semantic_2012,socher_recursive_2013,irsoy_deep_2014,tai_improved_2015,wieting_paraphrase_2015} or construct internal composition hierarchies on their own \autocite{zhao_self-adaptive_2015,chen_sentence_2015}. 

Another relevant work is the Paragraph Vector or doc2vec model \autocite{le_distributed_2014}. It builds directly on word2vec \autocite{mikolov_distributed_2013} to learn distributed representations of individual, arbitrarily long token sequences (paragraphs) while training word embeddings. The approach lends from the word2vec continuous bag of words (CBOW) model, but modifies its optimization procedure as follows. A \textit{virtual} word, that is shared along one paragraph, is added to every bag-of-word (BoW) context instance that belongs to this paragraph. Apart from that the training stays the same, i.e. using the average of all word embeddings related to one context of a target word, the system is optimized to predict this target. As soon as the model has converged, the vectors associated to the virtual words represent the respective paraphrase embedding. In fact, doc2vec is not a pure composition model as it requires to train the paragraph vectors along with the word embeddings. But it produces sequence embeddings that perform quite well for several semantic tasks \autocite{lau_empirical_2016}.


%\todo{AB:bi-gram composition not heavily affected by exploding-/vanishing gradient effects}


% composition of multi-word phrases & sentences 
% --> AVG vs LSTM (CNN?)
% --> use of dependency grammar information

% Compositional Distributional Semantic Models 
%additive \autocite{zanzotto_estimating_2010} \\

%\subsection{Composition models}
%survey \autocite{wang_comparison_2017}; \\
%survey on bi-gram composition (psych. ling. mot.) \autocite{mitchell_composition_2010} (e.g. semantic priming); \\
%survey on bi-gram and sentence comp (sum, mul, RecNN) \autocite{blacoe_comparison_2012} (avg better)
%German compounds (bi-gram) \autocite{dima_reverse-engineering_2015}; \\
%Skip-thought \autocite{kiros_skip-thought_2015}; \\
%RecNN w/ parse structure \autocite{socher_dynamic_2011,socher_semantic_2012,socher_recursive_2013,tai_improved_2015,wieting_paraphrase_2015}; \\ 
%RecNN (deep) \autocite{irsoy_deep_2014}; \\ 
%RecNN w/o predefined structure \autocite{zhao_self-adaptive_2015,chen_sentence_2015}; \\
%doc2vec \autocite{le_distributed_2014,lau_empirical_2016}; \\ 
%CNN for sentence modeling \autocite{kalchbrenner_convolutional_2014}; \\
%CNN \autocite{kim_convolutional_2014,hu_convolutional_2014,yin_convolutional_2015,he_multi-perspective_2015}; \\
%just word embeddings: bi-LSTM + char-embeddings \autocite{ling_finding_2015}; \\
%multi-LSTM \autocite{liu_multi-timescale_2015}; \\
%deep avg (DAN) \autocite{iyyer_deep_2015}; \\
%feature-weighted avg (FCT) \autocite{yu_learning_2015}; \\
%functional composition (ling motivated) \autocite{baroni_frege_2014,paperno_practical_2014}; \\
%intra- vs extra-sentential context (ling motivated) \autocite{polajnar_exploration_2015}; \\
%para + doc embeddings (fixed hierarchical LSTM, auto-encoder) \autocite{li_hierarchical_2015}; \\  
%multilingual \autocite{hermann_multilingual_2014}; \\

%AVG vs LSTM on paraphrase \autocite{wieting_towards_2015} % argues for and uses objective function of \textcite{tai_improved_2015}
%impact of word order \autocite{pham_sentence_2013}

%\subsection{Similarity prediction}
%traditional approaches; \\
%SemEval-2016 Task 1: STS \autocite{agirre_semeval-2016_2016}; \\
%SemEval-2017 Task 1: STS \autocite{cer_semeval-2017_2017}; \\

%siamese RNN + manhatten (simple) \autocite{mueller_siamese_2016}; \\
% applied to arguments: \autocite{habernal_exploiting_2015, boltuzic_identifying_2015, misra_measuring_2016};

%see \autocite{wieting_towards_2015}

%\autocite{iyyer_deep_2015}: "RecNNs can model complex linguistic phenomena like negation \autocite{hermann_not_2013}", names "syntactically-aware models" and "Word Dropout Improves Robustness"

%\autocite{turney_domain_2013}: "the model should be sensitive to the order of the words in a phrase (for composition) or a word pair (for relations), when the order affects the meaning"
