\section{Discussion and Future Work}

In line with \textcite{mueller_siamese_2016,iyyer_deep_2015}, our results demonstrate that simple neural models perform quite well for semantic aware composition. Furthermore, we investigated the relation of dependency type data and order aware composition. In this chapter we discuss our findings.% and conclude... 

As demonstrated in Chapter~\ref{subsec:results_relation_OA_DA} locally contextualized processing of individual tokens does matter. That capability is achieved by order aware \ac{RNN} models holding previously processed information in an internal state which is considered when analyzing the next token. By keeping the internal state reasonable in size, bottleneck effects induce localization. It suggests, that this kind of contextualization leverages performance by filtering relevant information similar to processes like word sense disambiguation\todo{UL: What?}. But it does not seem to be like sequential processing is a requirement. In addition to several well performing \ac{CNN} models, one might imagine to process every individual token in a bag of words manner, while incorporating a vector representation previously created from this bag of words, or in a n-grams fashion, eventually. This mechanism builds upon \textit{attention} \autocite{bahdanau_neural_2014, vaswani_attention_2017} and is called self-attention or intra-attention \autocite{cheng_long_2016}. It seems to perform well on semantic \ac{NLP} exploiting very little % or no\footnote{That is meant with respect to a certain area, e.g. a sentence, defining a context frame by itself. As reasoned above, \textit{any} contextualization is important.} 
ordering information \autocite{parikh_decomposable_2016}. In fact, that approach sets one or multiple context frames for each token which could be considered as ordering information, too. %, but despite of, provides a local context. 
Further experiments taking some of these insights into account could be arranged by simply shuffling the sentence tokens and repeating our experiments. By doing so, the context is artificially enlarged to cover the hole sentence and one might investigate, if \ac{RNN} models still outperform averaging of independently mapped embeddings. Likely this experimental setting requires to use Bi-LSTMs \autocite{graves_speech_2013} or similar \ac{RNN} architectures, that provide information regarding all previous and following tokens at every position. \todo{UL: Why such speculations?}

%One might expect, that the \acp{RNN} still outperform averaging as they allow contextualization in a manner that is more expressive, as t% and the impact of . % 

Even though all models examined in this work conceptually have access at comparison level\footnote{i.e. similarity measure application, see Section~\ref{subsec:architecture}} to all information included in the respective embeddings, intermediate sentence representations functioning as bottleneck constrain their outreach, which underpins the benefit of local filtering. To proof this idea, one could extend our models by adding eventually deep networks on top of the sentence embedding layer similar to \textcite{iyyer_deep_2015} and increase the dimensionality of sentence representations itself. Thus the performance advantage of the order aware models should decrease.\todo{UL: separate discussion from outlook (X)}

Furthermore, we argued that locally contextualized processing as exploited by order aware models can be achieved by adding dependency type information. As mentioned in \ref{subsec:dependency_types}, order information strongly correlates with dependency type data for English language. Our findings underpin this thesis. However, the performance is still behind the order aware case in general, but exploiting dependency types significantly outperforms applying order awareness in the presence of passive constructions. This insight leads to the question if there are other structural cases in which using dependency type data can outperform order aware processing. Passive is just one kind of \textit{syntactic alternation}\footnote{See \autocite{levin_english_1993} for a comprehensive study.}, among many others. Another example is the dative alternation in the following example\footnote{The example was taken from \autocite{kendall_dative_2011}.}:
%\begin{enumerate}[label=(\alph*)]
%	\setlength\itemsep{-.5em}
%	\item Who gave that wonderful watch to you?
%	\item Who gave you that wonderful watch?
%\end{enumerate}
\begin{exe}
	\ex \label{ex:alternation} 
	\begin{xlist}	
		\ex \label{ex:alternation_a} Who gave \underline{that wonderful watch} \underline{to you }?
		\ex \label{ex:alternation_b} Who gave \underline{you} \underline{that wonderful watch}?
	\end{xlist}
\end{exe}
Syntactical alternating phrases express the same meaning\footnote{That is true in a broader sense of meaning. Apparently, there is a subtle difference in means of focus.}, but alternate at least partly the way their arguments are expressed (e.g. by the alternation between a prepositional indirect-object construction in Example~\ref{ex:alternation_a} and a double-object construction in Example~\ref{ex:alternation_b}). One might expect that processing these constructions would benefit from dependency type data, too. In the field of corpus linguistics the dative alternation is heavily studied \autocite{lapata_acquiring_1999,bresnan_gradience_2003,bresnan2007predicting,kendall_dative_2011}. It should be straight forward to lend extraction patterns and create respective SICK corpus subsets to evaluate if dependency type data is also superior to ordering information in this case. Additionally, one might look into languages with less restrictive word order, like Turkish or Latin.

This work is based on the SICK corpus. As it is artificially created from image descriptions, it probably narrows the space of semantic and syntactic phenomena observed in nature. Furthermore, using a manually scored corpus requires a conceptional (pre-)determination of \textit{relatedness} potentially holding a bias. For instance, having a second look at the sentence tuple presented in Table~\ref{tab:sick_examples}:
%\begin{enumerate}[label=(\alph*)]
%	\setlength\itemsep{-.5em}
%	\item A woman is chopping an onion
%	\item A woman is washing her feet
%\end{enumerate}
\begin{exe}
	\ex \label{ex:sick} A woman is chopping an onion \\
		A woman is washing her feet
\end{exe}
This tuple is scored with a relatedness of $0.0$. However, one might expect that the following tuple scores even lower:
%\begin{enumerate}[label=(\alph*)]
%	\setlength\itemsep{-.5em}
%	\item A woman is chopping an onion
%	\item A man is washing his feet
%\end{enumerate}
\begin{exe}
	\ex \label{ex:no_sick} A woman is chopping an onion \\
		A man is washing his feet
\end{exe}
Of course, different segments of a sentence, like the verbal parts, can be more important to the total meaning then other. But denying the fact that the actor in Example~\ref{ex:sick} is identical, seems to be too restrictive. This could be an instance of priming \autocite{weingarten_primed_2016}, i.e., that reading one sentence influences the perception of a following one unconsciously. It leads to the question if relatedness expressed at the beginning of the sentences is systematically underrated in the SICK corpus.

We discarded the evaluation result in means of Pearson's $r$ because it shows same strange behavior that we traced back to a deviation bias along the parameter \texttt{order aware}. To determine the origin of these circumstances, further investigations are necessary. Especially, the impact of switching to a loss function based on Pearson correlation should be examined.


%ADD CONTENT

%strange Pearson scores


% Alternations / Diathese
%http://ling.uni-konstanz.de/pages/home/hautli/LR/verb-classes-levin.pdf
%http://ccl.pku.edu.cn/973_sem_spec/Sem_ling/English%20Verb%20Classes%20and%20Alternations%20A%20Preliminary%20Investigation.pdf

%context -> word sense disambiguation

% what does contextualized processing mean? when is it necessary?
% 1) needs "memory" (internal state)

% contextualization: related to attention?


% SICK is artificial: pros/cons?


% strange Pearson scores

%\section{Conclusion} \todo{UL: remove Conclusion}

%This work shows that simple neural composition models perform well for sentence relatedness prediction by implementing an \ac{RNN} based sequential approach and an averaging model that transforms the embeddings individually with a \ac{FC} layer. Our results are comparable with the best submissions for SemEval-2014 Task 1. However, recent models outperform these results.

%We demonstrated that composition based on sequential processing outperforms averaging of individually processed embeddings when keeping the amounts of trainable parameters equal. 

%Furthermore, we argued that dependency type data encodes context information that is important for semantic aware token composition. Despite sequential processing outperforms the sole usage of dependency type data, there are cases in which the later is superior, like in the presence of passive constructions. 

