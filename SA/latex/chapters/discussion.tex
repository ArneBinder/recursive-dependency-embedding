\section{Discussion and Future Work}

In line with \textcite{mueller_siamese_2016,iyyer_deep_2015}, our results demonstrate that simple neural models perform quite well for semantic aware composition. Furthermore, we investigated the relation of dependency type data and order aware composition. In this chapter we discuss our findings.% and conclude... 

As demonstrated in Chapter~\ref{subsec:results_relation_OA_DA} locally contextualized processing of individual tokens does matter. That capability is achieved by order aware \ac{RNN} models holding previously processed information in an internal state which is considered when analyzing the next token. By keeping the internal state reasonable in size, bottleneck effects induce localization. It suggests, that this kind of contextualization leverages performance by filtering relevant information similar to processes like word sense disambiguation. But it does not look like sequential processing is a requirement. In addition to several well performing \ac{CNN} models, one might imagine to process every individual token in a bag of words manner, but incorporating a vector representation previously created from this bag of words, or in a n-grams fashion, eventually. This mechanism builds upon \textit{attention} \autocite{bahdanau_neural_2014, vaswani_attention_2017} and is called self-attention or intra-attention \autocite{cheng_long_2016} that seems to perform well on semantic \ac{NLP} exploiting very little% or no\footnote{That is meant with respect to a certain area, e.g. a sentence, defining a context frame by itself. As reasoned above, \textit{any} contextualization is important.} 
ordering information \autocite{parikh_decomposable_2016}. In fact, that approach sets one or multiple context frames for each token which could be considered as ordering information, too.%, but despite of, provides a local context. 
Further experiments taking some of these insights into account could be arranged by simply shuffling the sentence tokens and repeating our experiments. By doing so, the context is artificially enlarged to cover the hole sentence and one might investigate, if \ac{RNN} models still strongly outperform averaging of independently mapped embeddings. Likely this experimental setting requires to use Bi-LSTMs \autocite{graves_speech_2013} or similar \ac{RNN} architectures, that provide information regarding all previous and following tokens at every position.

%One might expect, that the \acp{RNN} still outperform averaging as they allow contextualization in a manner that is more expressive, as t% and the impact of . % 

Even though all models examined in this work conceptually have access at comparison level\footnote{i.e. similarity measure application, see Section~\ref{subsec:architecture}} to all relevant information included in the respective embeddings, intermediate sentence representations functioning as bottleneck prevent that, which underpins the benefit of local filtering. To proof this idea, one could extend our models by adding eventually deep networks on top of the sentence embedding layer similar to \textcite{iyyer_deep_2015} and increase the dimensionality of sentence representations itself. Thus, the performance advantage of the order aware models should decrease.

Furthermore, we argued that locally contextualized processing as exploited \todo{AB: better?: "implemented"} by order aware models can be achieved by adding dependency type information. As mentioned in \ref{subsec:dependency_types}, order information strongly correlates with dependency type data for English language. Thus, 


ADD CONTENT

% Alternations / Diathese
%http://ling.uni-konstanz.de/pages/home/hautli/LR/verb-classes-levin.pdf
%http://ccl.pku.edu.cn/973_sem_spec/Sem_ling/English%20Verb%20Classes%20and%20Alternations%20A%20Preliminary%20Investigation.pdf

%context -> word sense disambiguation

% what does contextualized processing mean? when is it necessary?
% 1) needs "memory" (internal state)

% contextualization: related to attention?


% SICK is artificial: pros/cons?


% strange Pearson scores