OPEN:
    coding:
     * check "load lexicon_roots from /root/corpora_out/DBPEDIANIF/1000/merged_min100/forest.root.id" when continue training from checkpoint
    experiments:
     * GRID search/parameter selection
        * try: bigger learning rate with multiple dense layers
    data:
     * re-parse:
        * general:
            * add ID-LINK node that parents the id
            * add dependency edge types (process_sentence3?  or process_sentence10?)
            * add SECTION/ABSTRACT node as direct child of primary section (^=abstract)
        * BioASQ:
            * use pure PARAGRAPHs (with PARAGRAPH/BACKGROUND, etc. as direct children) TEMPORARY FIX: uniformed all
                paragraph ids to id of PARAGRAPH/BACKGROUND
            * (?) model paragraphs as sequence (?)
        * DBpediaNif:
            * add titles    NO: already available, but really sparse
    writing:
     * create structure / table of contents
    orga:
     * check "Halbkurse": what can be moved to "non-practical" (technical or theoretical)?
        * courses by Burkhard ("Intelligente Roboter", "Einführung in die künstliche Intelligenz")
        * "Einführung in Semantic Web" (Wandelt)
     * enrol for diploma thesis


DONE:
 * attention_SINGLE + identity - DONE
 * check number of structured abstracts in BioASQ dataset - DONE: ~3/10 ≈ 4.05 Mio abstracts
 * peek into prediction results on wikipedia - (DONE: looks confusing)
 * show token coverage when creating corpora (merging),
   evtl. introduce a parameter `coverage` that can replace `min_count`
 * enable usage of additional embeddings, i.e. biomedical embeddings
 * check multi-run setting (early stopping, ...) DONE (implemented caching for lexicon and trees)
 * implemented caching to fasten repeated execution of the same setting
 * check for overfitting on small dataset (local)
   DONE: it does, after some epochs (FLATGRU:~290;FLATconcat_GRU:~160;FLATconcat_BIGRU:~170, but: HTU:~60)
 * neg_sampling: sample from lexicon and calc probs from frequencies
 * early_stopping_metric as parameter
 * re-add testing for language model (reroot) to see performance with dropout disabled


NOTES:
 * possible Zweitbetreuer: Manfred Stede (NO), Roland (OK)