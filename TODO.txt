OPEN:
    coding:
     * (-) check "load lexicon_roots from /root/corpora_out/DBPEDIANIF/1000/merged_min100/forest.root.id" when continue training from checkpoint
     * (-) training: restrict link targets to selected train/test data
     * (-) fix double assignment bug (0 can not be a fixed vec, see TreeEmbedding.embed and Lexicon.transform_idx)
    experiments:
     * GRID search/parameter selection
        * try: bigger learning rate with multiple dense layers (BioASQ)
     *
    data:
     * re-parse:
        * general:
            * add ID-LINK node that parents the id
            * add dependency edge types (process_sentence3?  or process_sentence10?)
            * add SECTION/ABSTRACT leaf node as direct child of primary section (^=abstract)
        * BioASQ:
            * use pure PARAGRAPHs (with PARAGRAPH/BACKGROUND, etc. as direct children (leaf))
                TEMPORARY FIX (DONE): uniformed all paragraph ids to id of PARAGRAPH/BACKGROUND
                    RE-DO! corpus was newly merged recently
            * (?) move title out of section
            * (??) model paragraphs as sequence
        * DBpediaNif:
            * add titles    NO: already available, but really sparse    NOAGAIN: these are only section titles!
     * check:
    writing:
     * create structure / table of contents
    orga:


DONE:
 * attention_SINGLE + identity - DONE
 * check number of structured abstracts in BioASQ dataset - DONE: ~3/10 ≈ 4.05 Mio abstracts
 * peek into prediction results on wikipedia - (DONE: looks confusing)
 * show token coverage when creating corpora (merging),
   evtl. introduce a parameter `coverage` that can replace `min_count`
 * enable usage of additional embeddings, i.e. biomedical embeddings
 * check multi-run setting (early stopping, ...) DONE (implemented caching for lexicon and trees)
 * implemented caching to fasten repeated execution of the same setting
 * check for overfitting on small dataset (local)
   DONE: it does, after some epochs (FLATGRU:~290;FLATconcat_GRU:~160;FLATconcat_BIGRU:~170, but: HTU:~60)
 * neg_sampling: sample from lexicon and calc probs from frequencies
 * early_stopping_metric as parameter
 * re-add testing for language model (reroot) to see performance with dropout disabled
 * bioasq-corpus: use PARAGRAPH instead of PARAGRAPH/... (to "sync" with min_100_paragraph data)
 * constants: check, if renaming affects any current data DONE (fixed issues) -> merge (check again)
 * embedding_api: blank IDs only, if reroot model (makes them visible if a simple corpus without model is loaded)
 * (!) FIX ROOT_IDS: freeze is important after convert_hashes_to_indices because len(lexicon) is added to root ids
        -> use negative values for root_ids AND use lexicon_roots
 * training: allow index files for reroot model to allow training on a data subset
 * merge lexicon_roots when concatenating forests
 * training: check merge lexicon for fine-tuning (load from previous checkpoint)
 * training: allow fine-tuning of just the embedding-model and the complete model
 * check data: depths of all positions (dbpedanif: 14+-5; bioasq: 16+-5)
 * added parameter all_vecs_fixed
 * added parameter dont_dump_trees
 * training: create TFIDF vocabulary only from train data and use it without expansion for test data creation
 * training: tree-model: fix halting after "DEBUG create trees with concat_mode=tree" for large datasets
        -> NOT FIXED, but reason is lac of memory usage of swap
 * training: implement TFIDF+HTU embeddings
 * check "Halbkurse": what can be moved to "non-practical" (technical or theoretical)?
    * courses by Burkhard ("Intelligente Roboter", "Einführung in die künstliche Intelligenz")
    * "Einführung in Semantic Web" (Wandelt)
 * enrol for diploma thesis




NOTES:
 * possible Zweitbetreuer: Manfred Stede (NO), Roland (OK)


ASSUMPTIONS/SPECIFICATIONS:
 * data-structure:
    * OFFSET_ID = 1; OFFSET_CONTEXT = 2; OFFSET_SEEALSO_ROOT = 3 (see constants)
    * last child of root points to CONTEXT (beginning of textual content):
        see collect_root_context_sizes:
            root_context_sizes = (root_length - (root_seealso_counts * 2 + 1)) - OFFSET_SEEALSO_ROOT
    * 3 states of data:
        (0) hashes
        (1) lexicon indices: positive indices for DATA, indices for IDs are negative and shifted by 1 (to handle 0 idx)
        (2) lex_var and lex_fixed indices, eventually INVERTED:
                lex_var positive, lex_fixed negative; +(-) len(lexicon) for INVERTED
    * (0) -> (1) convert_data_hashes_to_indices: ON CORPUS CREATION
        hash
            -> IF IN id_offset_mapping (from root_ids): len(lexicon) + id_offset_mapping[d]
            -> ELSE: idx (lexicon, by mapping)
    * (1) -> (2) transform_idx: ON RUNTIME (TRAINING/PREDICTION/VISUALIZATION)
        NOTE: removes/resolves negative lexicon indices (used for IDs in state (1))
        idx -> ((idx_var OR idx_fixed (by mapping dicts)) ( -> IF REVERT: -idx_var OR -idx_fixed)) OR UNKNOWN(_replacment)


