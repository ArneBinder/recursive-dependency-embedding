# change this!
HOST_CORPORA_OUT=/home/abinder/corpora
HOST_TRAIN=/mnt/hdd/experiments/abinder/recursive-embedding/train

## general settings
HOST_PORT_NOTEBOOK=8887
HOST_PORT_TENSORBOARD=6005

## general resource limitations
MEM_LIMIT=150g
CPU_SET=0-3
# disabled, because it overwrites parameter passed to train.sh
#NVIDIA_VISIBLE_DEVICES=0

## cpu / gpu version
# used to choose the dockerfile (Dockerfile.<DOCKERFILE_EXT>) and the image tag (tensorflowfold:<DOCKERFILE_EXT>)
#DOCKERFILE_EXT=tf1_3_cpu_mkl
DOCKERFILE_EXT=tf1_3_gpu
#DOCKER_RUNTIME=runc
DOCKER_RUNTIME=nvidia

## index files
TRAIN_FILES=forest.idx.train.0.npy,forest.idx.train.1.npy,forest.idx.train.2.npy,forest.idx.train.3.npy,forest.idx.train.4.npy,forest.idx.train.5.npy,forest.idx.train.6.npy,forest.idx.train.7.npy,forest.idx.train.8.npy,forest.idx.train.9.npy
#DONT_TEST=True
#TRAIN_FILES=forest.idx.0.npy,forest.idx.1.npy
# If RUN_COUNT >= 1 (multiple run execution) a semicolon marks DEV_FILE_INDICES splits.
# This can be used to execute cross-fold training, e.g. including the following line enables 5-fold cross evaluation
# assuming 10 train index files
DEV_FILE_INDICES=0,1;2,3;4,5;6,7;8,9
# setting TEST_FILES with multi run setting results in still using DEV_FILE_INDEX of TRAIN_FILES for early stopping
TEST_FILES=forest.idx.test.0.npy

## Multiple run execution: triggered by RUN_COUNT >= 0!
#GRID_CONFIG_FILE=parameters.jl
RUN_COUNT=1


