

20-04-18
========
 * fixed (optimized, now usable) train_iter for concat_mode=aggregate
 * (inner) batching in batch_iter_all

22-04-18
========
 * filter out List_of_... dbpedianif articles (source + target roots):
     max size of tokens per article decreases from >11k to 5k
 * added length threshold for concat_mode==aggregate (default: 1k) in tree_iterator
 * fixed batch_iter_nearest for DummyTreeModel (tfidf)
 * note: batch_iter_nearest can lead to crashes because normed_embeddings stays in gpu memory

24-04-18
========
 * use softmax over candidates in TreeTupleModel_with_candidates
 * normalize gold_probs in max entropy calculation of TreeTupleModel_with_candidates (and return un-normalized for eval)
 * embedding_api: prepared integration of TreeTupleModel_with_candidates usage

25-04-18
========
 * create model_nearest only if batch_iter_nearest is used (for test/train)
 * implemented embedding_api endpoint /tuple_scores
 * TreeTupleModel_with_candidates: return sigmoid (instead softmax) of logits as values_predicted

26-04-18
========
 * restrict gpus via docker-compose: set NVIDIA_VISIBLE_DEVICES explicitly as environment variable (via "ENV" in docker
   file or "environment" section in compose file)
 * batching for calc_tuple_scores (embedding_api)
 * entangled embedding calculation and scoring to enable caching of embeddings (calc_tuple_scores)

29-04-18
========
 * modularized execute_run and use relevant parts in embedding_api
 * added metrics

30-04-18
========
 * removed ids from indices iterators and translated ids_target to indices_targets e.g. use only indices pointing into
   forest data
