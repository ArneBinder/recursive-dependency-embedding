\section{Introduction} %\todo{(AG): Dreiteilung: (1) Reserachkontext / Topaufgabe = clustering similar arguments (ok) (2) Was gibt es schon.  Welche Probleme gibt es schon? Motivation: z.B. aus Misra: "averaging word2vec embeddings for each
%word may lose too much information in long sentences."(3) Mein wissenschaftlicher Beitrag ()}

Large online debates\todo{UL:add examples, numbers} contain redundant arguments and therefore hinder efficient debating.
To tackle this problem, \Textcite{boltuzic_identifying_2015} proposed to cluster similar arguments and filter only the most representative ones.\todo{UL:explain further (intention?)} The Common Round\footnote{see \url{http://commonround.dfki.de/}} project \autocite{uszkoreit_common_2017} introduces a platform to facilitate large scale online debating with \ac{NLP} technologies. The authors define the aggregation of the semantic content of debates as one of their major objectives which they intend to achieve by clustering similar arguments.
Moreover, \Textcite{misra_measuring_2016} define the \ac{AFS} task as the recognition of the semantic similarity of two sentential arguments.\todo{AB:move away from arguments as motivation!}

%\vspace{11pt}
%\bigskip
%\noindent
To achieve these\todo{UL:more precise!} goals, a semantically consistent similarity measure for arguments is necessary. Frameworks like word2vec \autocite{mikolov_distributed_2013} or glove \autocite{pennington_glove_2014} lead to success in many \ac{NLP} tasks by producing dense embedding vectors for single words. The cosine distance of these word embeddings enables a semantically consistent similarity measure for word tokens. Building on this, \Textcite{habernal_exploiting_2015,boltuzic_identifying_2015,misra_measuring_2016} use summed or averaged word embeddings for argument similarity measures. 
However, \Textcite{misra_measuring_2016} state that averaging all word embeddings may lose too much information in long sentences. \Textcite{wang_comparison_2017} present an overview of different embedding composition models for phrase representations. They conclude that the recurrent \ac{LSTM} model \autocite{hochreiter_long_1997} just slightly outperforms the additive baseline model in this task. \Textcite{tai_improved_2015} introduced the recursive TreeLSTM model which further reduced the vanishing gradient problem as compared to recurrent models by shortening the average distance of entities in the computation graph. The authors use dependency parse trees to construct the neural model and present promising results for the SICK phrase relatedness task\footnote{The SICK corpus \autocite{marelli_sick_2014} contains $\sim$10.000 similarity scored sentence pairs. The system by \Textcite{tai_improved_2015} achieved a Pearson's $r$ of 0.8676 when predicting the similarities.}. 

%\vspace{11pt}
%\bigskip
%\noindent
However, these models\todo{UL:specify!} are applied to sentences, but not yet to multi-sentence arguments or paragraphs. Additionally, their\todo{UL:specify!}  approach does not use dependency edge type information such as 'nsubj' (nominal subject) or 'prep' (preposition).

\subsection{Objective}
In this work we examine what degree of semantical awareness is achievable with different existing embedding composition models of varying complexity when applied to sentences\todo{UL:clarify sentence level vs. super-sentence level mismatch}. We regard compositions of token embeddings as semantically aware if they yield a distance measure that matches human intuition. Furthermore, our goal is to provide access to the rapid implementation of different neural network topologies including \ac{RecNN} for semantically aware composition.

\subsection{Approach}
To achieve these goals we implement the following: (a) an averaging model, and (b) a neural sequence model. We evaluate the semantical awareness with the SICK \ac{STS} challenge \autocite{marelli_sick_2014} and compare the model performances against a \acs{TF-IDF} baseline.

To prepare for rapid comparison with various neural models, we use the Tensorflow Fold\footnote{see \url{https://github.com/tensorflow/fold}} framework \autocite{looks_deep_2017} for implementation. It allows fast training of neural networks with dynamic computation graphs, i.e. networks whose structure depends on the data that is fed during training. Moreover, it is based on Tensorflow\footnote{see \url{www.tensorflow.org}} which is widely used as a deep learning framework.

%Furthermore, we tackle the data sparsity problem by examining pre-training capabilities 

%\subsection*{Model details}
As neural sequence model, we use an approach based on \Textcite{mueller_siamese_2016} due to its simplicity. It consists of one LSTM as composition model which is applied to two input sentences and uses the Manhattan metric as distance measure. Nevertheless, the authors report a Pearson's $r$ of 0.8822 for the SICK challenge. Furthermore, we analyze the effect of dependency type information by optionally appending edge type embeddings\todo{UL:explain} to the token embedding vectors. 

%\subsection*{Dataset enlargement}
One major drawback of the SICK corpus is its small size. We will use pre-training with paraphrase data from the PPDB\footnote{see \url{http://paraphrase.org}} corpus \autocite{ganitkevitch_ppdb_2013} and evaluate its effect to the model performances. In its smallest, most accurate version the phrasal subset of the corpus contains 1,530,812 pairs of short phrase snippets like ('\texttt{maintain international peace and}', '\texttt{maintaining world peace and}'). We treat these pairs as similar and extend the dataset with artificially generated negative samples where the two phrases are randomly sampled from the corpus.
