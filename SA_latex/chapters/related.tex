\section{Related work}

%\subsection{Composition models}
survey \autocite{wang_comparison_2017}; \\
survey on bi-gram composition (psych. ling. mot.) \autocite{mitchell_composition_2010} (e.g. semantic priming); \\
survey on bi-gram comp (sum, mul, RecNN) \autocite{blacoe_comparison_2012} (avg better)
German compounds (bi-gram) \autocite{dima_reverse-engineering_2015}; \\
Skip-thought \autocite{kiros_skip-thought_2015}; \\
RecRNN w/ parse structure \autocite{socher_dynamic_2011,socher_semantic_2012,socher_recursive_2013,tai_improved_2015,wieting_paraphrase_2015}; \\ 
RecRNN (deep) \autocite{irsoy_deep_2014}; \\ 
RecRNN w/o predefined structure \autocite{zhao_self-adaptive_2015,chen_sentence_2015}; \\
doc2vec \autocite{le_distributed_2014,lau_empirical_2016}; \\ 
CNN on bag of words \autocite{kalchbrenner_convolutional_2014}; \\
CNN \autocite{kim_convolutional_2014,hu_convolutional_2014,yin_convolutional_2015,he_multi-perspective_2015}; \\
bi-LSTM + char-embeddings \autocite{ling_finding_2015}; \\
multi-LSTM \autocite{liu_multi-timescale_2015}; \\
deep avg (DAN) \autocite{iyyer_deep_2015}; \\
feature-weighted avg (FCT) \autocite{yu_learning_2015}; \\
functional composition (ling motivated) \autocite{baroni_frege_2014,paperno_practical_2014}; \\
intra- vs extra-sentential context (ling motivated) \autocite{polajnar_exploration_2015}; \\
para + doc embeddings (fixed hierarchical LSTM, auto-encoder)
\autocite{li_hierarchical_2015}; \\  
multilingual \autocite{hermann_multilingual_2014}; \\

AVG vs LSTM on paraphrase \autocite{wieting_towards_2015}
impact of word order \autocite{pham_sentence_2013}

%\subsection{Similarity prediction}
traditional approaches; \\
SemEval-2016 Task 1: STS \autocite{agirre_semeval-2016_2016}; \\
SemEval-2017 Task 1: STS \autocite{cer_semeval-2017_2017}; \\

siamese RNN + manhatten (simple) \autocite{mueller_siamese_2016}; \\

\autocite{habernal_exploiting_2015};
\autocite{boltuzic_identifying_2015};
\autocite{misra_measuring_2016};

%see \autocite{wieting_towards_2015}


