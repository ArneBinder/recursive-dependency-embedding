%\section*{Abstract}

\begin{center}
	\huge{Abstract} 
	\vspace{0.5cm}
	 
	\large{\bf{Training recursive compositional models with hierarchical linguistic information for semantic tasks in NLP}} 
	\vspace{0.5cm}

	%\hspace{10pt}
	% Author names and affiliations
	%\large
	by Arne Binder \\
	%\vspace{0.5cm}
	%\small  
	%$^1$) First affiliation\\
	%arthur.author@correspondence.email.com\\
	%$^2$) Second affiliation
\end{center}

%\hspace{10pt}
\vspace{0.5cm}
\normalsize


\if false
===OLD SRP CONTENT===
\acfp{VSM} for textual data lead to success in many \ac{NLP} tasks. Recently, prediction based word embedding models like word2vec %\autocite{mikolov_efficient_2013}
gained attention. These models build upon Distributional Semantics, i.e. a word is defined by its contexts, and scale up to billions of training tokens % \autocite{mikolov_distributed_2013}
resulting in robust embeddings for individual words. 
\acfp{CDSM} intend to create vector representations for sequences of tokens by composing word embeddings in a meaningful manner. However, it is up to debate which composition functions perform well for semantic tasks.

In this work, we study the impact of order aware processing to token embedding composition at sentence level by implementing (1) an averaging model and (2) a \ac{LSTM} based approach. Furthermore, we analyze the relation of order aware composition to syntactical information. We evaluate our models at the SICK relatedness prediction task.% \autocite{marelli_sick_2014}.

Our results underpin the thesis, that order aware processing is useful for semantic aware composition and subsumes syntactical information in most cases. However, there are instances of linguistic constructions in which syntactical information seems to be superior to order aware processing, namely in the presence of passive.
\fi
