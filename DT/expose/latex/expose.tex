%\section*{Abstract}

\begin{center}
	\huge{Expos\'e} 
	\vspace{0.5cm}
	 
	\large{\bf{The impact of structural complexity on semantic aware composition of linguistic tokens with respect to input length}} 
	\vspace{0.5cm}

	%\hspace{10pt}
	% Author names and affiliations
	%\large
	by Arne Binder \\
	%\vspace{0.5cm}
	%\small  
	%$^1$) First affiliation\\
	%arthur.author@correspondence.email.com\\
	%$^2$) Second affiliation
\end{center}

%\hspace{10pt}
\vspace{0.5cm}
\normalsize

\acfp{VSM} for textual data lead to success in many \ac{NLP} tasks. Recently, prediction based word embedding models such as word2vec %\autocite{mikolov_efficient_2013}
have been gaining attention. These models build upon Distributional Semantics, i.e. a word is defined by its contexts, and they scale up to billions of training tokens % \autocite{mikolov_distributed_2013}
resulting in robust embeddings for individual words. 
\acfp{CDSM} intend to create vector representations for sequences of tokens by composing word embeddings in a meaningful manner. However, it is up to debate which composition functions perform well for semantic tasks. \acp{RNN} produce appropriate results for short to medium length textual input on several semantic tasks \autocite{vinyals_show_2014,wu_googles_2016,xiong_microsoft_2017} as they are capable of contextualized token processing, but still suffer to handle long range dependencies and do not scale very well with text size\todo{CITATIONS}. Then again, averaging composition models such as fastText \autocite{joulin_bag_2017} demonstrate that avoiding any explicit structure may perform quite well at least for short texts like sentences. These flat models are very beneficial in means of training time and memory consumption, therefore enabling for huge amounts of training data. 

\acp{RecNN} generalize \acp{RNN} by allowing arbitrary trees instead of sequences as input structure. The consequences of this generalization are twofold: The distances of tokens that are potentially related\footnote{regarding the interpretation process} may decrease with respect to the computation graph. This may lead to better contextualization and, consequently, more precise interpretations of individual tokens. On the other hand, \acp{RecNN} introduce another degree of structural complexity and require predefined input structures. 

In this work, we study the impact of structural complexity on semantic aware composition of linguistic tokens. Especially, we investigate if it is possible to combine the best of both worlds, the speed and size of flat models and the context awareness of sequence models, by using tree structured token composition. We analyze the performance of tree structured, or \acp{RecNN}, models and compare it with structure less, or flat, models and sequence, or \acp{RNN}, models with respect to input length.

To achieve these goals, we implement the following composition models: 1) a flat summation model, 2) a \ac{GRU} sequence model, and 3) a tree structured \ac{RecNN} model. For the later, we keep the model as simple as possible and de-construct the tree composition unit into a mapping and reduction function that lend from the other two models for comparability: We use a \ac{GRU} step for mapping and apply summation to reduce an arbitrary number of embeddings to a single one. By doing so, feeding structure to the tree model that equals one of the edge cases (i.e. the degenerated trees: sequence or depth one tree) the model strongly resembles one of the comparison models. Furthermore, the separation in map and reduce functions allows to experiment if reduction should precede the mapping or the other way around. The former would reduce computational costs as one \ac{GRU} step is applied only once per tree unit application. The latter should allow more precise contextualization, but results in as many \ac{GRU} step executions as for the sequence model. Recently, the attention mechanism \autocite{bahdanau_neural_2014,xu_show_2015} was successfully applied in neural \ac{NLP} tasks \autocite{zhuang_neobility_2017,vaswani_attention_2017} and gained attention due to its simplicity. We will test whether the tree model will benefit from using attention as reduction function leading to a hierarchical attention model similar to \textcite{yang_hierarchical_2016}, but in a dynamic fashion. Moreover, we try to exploit one main feature of web content: Trees are natively expandable by other trees that are \textit{linked} to any node or token, respectively. Following links while constructing the tree structures may lead to useful subtrees for tokens that do not exist in the dictionary. Finally, we create \ac{TF-IDF} embeddings as baseline.   

	As argued in \textcite{binder_comparison_2018} with regard to the Distributional Hypothesis \autocite{harris_distributional_1954} semantic relatedness \autocite{resnik_semantic_1999, budanitsky_evaluating_2006} prediction is one fundamental task to evaluate semantic vector space. Although there are well curated relatedness-labeled datasets at paraphrase and sentence level \autocite{pavlick_ppdb_2015, dolan_automatically_2005, marelli_sick_2014,cer_semeval-2017_2017}, there is a lack of super-sentence relatedness corpora. As we are especially interested in scaling beyond sentence boundaries we seek to circumvent this shortcoming by exploiting interlinking information in Wikipedia articles. We heuristically take an article that is mentioned in the \textit{See Also} section of another one as semantically related to that article. We use this link prediction task to train and evaluate our embedding models. Taking the English portion of Wikipedia results in a dataset of $\sim$1 million documents\footnote{of a total of $\sim$5 million English articles} that occur in at least one of these links. For computational reasons we restrict the documents to the article abstracts. To bypass as much preprocessing hurdles as possible we make use of the DBpedia NIF \autocite{dojchinovski_dbpedia_2018} dataset\footnote{\url{http://wiki.dbpedia.org/dbpedia-nif-dataset}}. It consists of cleaned, plain Wikipedia article text, but enhanced with structural information extracted from Wikipedia HTML data such as annotations for sections, paragraphs and titles or anchors for intra-Wikipedia links. We will use this structural data in combination with dependency parse information to dynamically construct the tree model.


%TODO: only abstracts
%TODO: mention language model?

%three composition model types with respect to input size: 1) structure less or flat models, 2) sequence or \acp{RNN} models, and 3) tree structured or \acp{RecNN} models.



%tree structured composition models can outperform sequence models and summation models in terms of accuracy or training time. 

%models: tfidf, gru, sum, tree (reduce->map; gru + sum)
%further inventions: just for model==tree: following links, revert (map->reduce), attention (for reduce)
 

%order aware processing to token embedding composition at sentence level by implementing (1) an averaging model and (2) a \ac{LSTM} based approach. Furthermore, we analyze the relation of order aware composition to syntactical information. We evaluate our models at the SICK relatedness prediction task.% \autocite{marelli_sick_2014}.

%Our results underpin the thesis, that order aware processing is useful for semantic aware composition and subsumes syntactical information in most cases. However, there are instances of linguistic constructions in which syntactical information seems to be superior to order aware processing, namely in the presence of passive.

