%\section*{Abstract}

\begin{center}
	\huge{Expos\'e} 
	\vspace{0.5cm}
	 
	\large{\bf{The impact of structural complexity on Compositional Distributional Semantics Models 
	%semantic aware composition of word embeddings 
	%with regard to document size
			}} 
	\vspace{0.5cm}

	%\hspace{10pt}
	% Author names and affiliations
	%\large
	by Arne Binder \\
	%\vspace{0.5cm}
	%\small  
	%$^1$) First affiliation\\
	%arthur.author@correspondence.email.com\\
	%$^2$) Second affiliation
\end{center}

%\hspace{10pt}
\vspace{0.5cm}
\normalsize

\acfp{VSM} for textual data \autocite{salton_vector_1975} led to success in many \ac{NLP} tasks %\todo{UL: Zu generisch. Vergangenheit? (led)? Are the basis of many success stories ...?} 
such as information retrieval \autocite{dierk_smart_1971,deerwester_indexing_1990}, document classification and clustering \todo{AB: CITATION}, synonym detection \autocite{rapp_word_2003}, analogy question answering \autocite{turney_similarity_2006} and are capable of modeling experimental results in cognitive science \autocite{landauer_solution_1997,mcdonald_testing_2001}.
%general purpose word embeddings \autocite{lund_producing_1996}. 
Recently, prediction based word embedding models like word2vec \autocite{mikolov_efficient_2013} have been gaining attention. %\todo{UL: Zhg zum ersten Satz herstellen}
The underlying training methods build upon Distributional Semantics, i.e. a word is defined by its contexts, and scale up to %\todo{UL: Wie scaled ein Modell? Ein Algorithmus skaliert. Bezug unklar} 
corpora of billions of training tokens \autocite{mikolov_distributed_2013} resulting in robust embeddings for individual words. 
\acfp{CDSM} are \acp{VSM} that intend to create vector space representations for sequences of tokens by composing word embeddings in a meaningful manner. 

\acp{CDSM} based on \acp{RNN} produce appropriate results for short to medium length textual input on several semantic tasks like image caption generation %\todo{UL: Was ist ein semantic task? Lieber konkret die Tasks benennen} 
\autocite{vinyals_show_2014}, machine translation \autocite{wu_googles_2016} %or speech recognition \autocite{xiong_microsoft_2017} 
since they are capable of contextualized token processing, but still fail to handle long range dependencies as they suffer from vanishing gradients. %and do not scale \todo{UL: Wieder - was skaliert?} very well with text size\todo{AB: CITATION}. 
Furthermore, they are computational expensive\todo{AB: CITATION}. On the other hand, averaging composition models such as fastText \autocite{joulin_bag_2017} demonstrate that avoiding any explicit structure like word order information %\todo{UL: Structure wo - im Modell?} 
may also perform quite well at least for short texts. These flat models are very advantageous in means of training time and memory consumption, therefore enabling for huge amounts of training data, but fail to capture more complex semantic interactions like negation  \todo{AB: CITATION}, especially for larger documents. However, it is up to debate which composition functions perform well for multi-sentence documents while remaining as precise as for short input.%well for semantic tasks
%\todo{UL: "semantic tasks?" Unklar. Vielleicht "performs best for which task"?} 
%\todo{UL: Aber skalieren die anderen Modelle laut Aussagen oben nicht auch sehr gut? Widersprüchliche Argumentation.}

\acp{RecNN} \autocite{goller_learning_1996,socher_parsing_2011} generalize \acp{RNN} by allowing arbitrary trees as input instead of just linear sequences. The consequences of this generalization are twofold: The distances of tokens that are potentially related\footnote{regarding the interpretation process} may decrease with respect to the input graph. This may lead to easier contextualization and, consequently, more precise interpretations of individual tokens. Furthermore, by using functions like averaging or summation to compose child embeddings at every tree node, computation cost should decrease. On the other hand, \acp{RecNN} introduce another degree of structural complexity and require pre-calculated input structures.%\todo{UL: Unklar. Oben steht arbitrary trees, hier nun predefiniert.} 

In this work, we study the impact of structural complexity on \acp{CDSM}. Especially, we investigate if it is possible to combine the best of both worlds, the speed and size of flat models and the context awareness of sequence models, by using tree structured embedding composition. By doing so, it may be possible to enable semantic aware embedding calculation for a large range of text sizes. In that sense we analyze the performance of tree structured models and compare it with structure less and sequence models with regard to document size.


To achieve these goals, we implement the following composition models: 1) a flat summation model, 2) a \ac{GRU} \autocite{cho_properties_2014} sequence model, and 3) a tree structured \ac{RecNN} model. For the latter, we keep the model as simple as possible and de-construct %\todo{UL: restrict the trees to size 2 (stimmt doch, oder?) by implementing ...} 
the \ac{RecNN} unit into a reduction and mapping function that lend from the other two models for comparability: We use summation to reduce an arbitrary number of embeddings to a single one and execute a \ac{GRU} step to incorporate the current word embedding. If structure that equals one of the edge cases (i.e. the degenerated trees: sequence or depth one tree) is fed to this tree model, it strongly resembles one of the competitor models. Furthermore, the separation in map and reduce functions allows to experiment if reduction should precede the mapping or the other way around. Depending on its implementation, the former can reduce computational costs as the \ac{GRU} step may be executed only once per inner tree node. The latter should allow more precise contextualization, but results in as many \ac{GRU} step executions as for the sequence model. Recently, the attention mechanism \autocite{bahdanau_neural_2014,xu_show_2015} was successfully applied in neural \ac{NLP} tasks \autocite{zhuang_neobility_2017,vaswani_attention_2017} and gained attention due to its simplicity. We will test whether the tree model will benefit from using attention as reduction function leading to a hierarchical attention model similar to \textcite{yang_hierarchical_2016}, but in a dynamic fashion. %Moreover, we try to exploit one main feature of web content\todo{UL: Wieso denn Web? Das wurde noch nirgends erwähnt}: Trees are natively expandable by other trees that are \textit{linked} \todo{UL: Wo sollen diese Links herkommen? Unklar. Welche KB könnte da eingebunden werden?} to any node or token, respectively. Following links while constructing the tree structures may lead to useful subtrees for tokens that do not exist in the dictionary. 
Finally, we create \ac{TF-IDF} embeddings as baseline. 

As argued in \textcite{binder_comparison_2018} with regard to the Distributional Hypothesis \autocite{harris_distributional_1954} semantic relatedness \autocite{resnik_semantic_1999, budanitsky_evaluating_2006} prediction is one fundamental task to evaluate semantic vector space. Although there are well curated relatedness-labeled datasets at paraphrase and sentence level \autocite{pavlick_ppdb_2015, dolan_automatically_2005, marelli_sick_2014,cer_semeval-2017_2017}, there is a lack of super-sentence relatedness corpora. As we are especially interested in scaling beyond sentence boundaries %\todo{UL: Wieso? Wurde noch nicht beschrieben.} 
we seek to circumvent this shortcoming by exploiting interlinking information in Wikipedia articles. We heuristically take an article that is mentioned in the \textit{See Also} %\todo{UL:Uff. Also ... sehr  gewagt. Vielleicht einen kleinen kuratierten Subset davon bauen? "see also" kann wirklich alles sein und komplett unrelated.} 
section of another one as semantically related to that article. We use this link prediction task to train and evaluate our embedding models. Taking the English portion of Wikipedia results in a dataset of $\sim$1 million documents\footnote{of a total of $\sim$5 million English articles} that occur in at least one of these links. For computational reasons we restrict the documents to the article abstracts. To bypass as much preprocessing hurdles as possible we make use of the DBpedia NIF \autocite{dojchinovski_dbpedia_2018} dataset\footnote{\url{http://wiki.dbpedia.org/dbpedia-nif-dataset}}. It consists of cleaned, plain Wikipedia article text, but enhanced with structural information extracted from Wikipedia HTML data such as annotations for sections, paragraphs and titles or anchors for intra-Wikipedia links. We will use this structural data in combination with dependency parse information to dynamically construct the tree model.

Finally, we evaluate the resulting embedding models on the BioASQ Task A challenge\footnote{\url{http://participants-area.bioasq.org/general_information/Task6a/}}. This real world task requires to predict the Medical Subject Headings (MeSH)\footnote{\url{https://www.nlm.nih.gov/mesh/}} assigned to a PubMed abstract. As we focus on the impact of structure to composition, we restrict the BioASQ dataset to the subset of \textit{structured abstracts}\footnote{\url{https://www.nlm.nih.gov/bsd/policy/structured_abstracts.html}}. These PubMed abstracts are separated into labeled paragraphs and represent approximately one third of the total BioASQ dataset\footnote{The BioASQ 2018 dataset consists of $\sim$13.5 million documents}. We analyze the performance for embedding models trained solely on Wikipedia by feeding their result into a simple classification model. Furthermore, we train the full model stack on BioASQ data only and, finally, try to fine-tune the Wikipedia embedding models on BioASQ.  



%TODO: USE MeSH Indexing TO EVALUATE!!! \autocite{mao_mesh_2017} 
% Competition: http://participants-area.bioasq.org/general_information/Task6a/

%TODO: re-add "links" or other sparse data (knowledge base)?

%TODO: mention language model?

%TODO: use hints from socher_recursive_2013 (Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank)


%three composition model types with respect to input size: 1) structure less or flat models, 2) sequence or \acp{RNN} models, and 3) tree structured or \acp{RecNN} models.



%tree structured composition models can outperform sequence models and summation models in terms of accuracy or training time. 

%models: tfidf, gru, sum, tree (reduce->map; gru + sum)
%further inventions: just for model==tree: following links, revert (map->reduce), attention (for reduce)
 

%order aware processing to token embedding composition at sentence level by implementing (1) an averaging model and (2) a \ac{LSTM} based approach. Furthermore, we analyze the relation of order aware composition to syntactical information. We evaluate our models at the SICK relatedness prediction task.% \autocite{marelli_sick_2014}.

%Our results underpin the thesis, that order aware processing is useful for semantic aware composition and subsumes syntactical information in most cases. However, there are instances of linguistic constructions in which syntactical information seems to be superior to order aware processing, namely in the presence of passive.

