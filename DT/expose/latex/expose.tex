%\section*{Abstract}

\begin{center}
	\huge{Abstract} 
	\vspace{0.5cm}
	 
	\large{\bf{The impact of structural complexity on semantic aware composition of linguistic tokens}} 
	\vspace{0.5cm}

	%\hspace{10pt}
	% Author names and affiliations
	%\large
	by Arne Binder \\
	%\vspace{0.5cm}
	%\small  
	%$^1$) First affiliation\\
	%arthur.author@correspondence.email.com\\
	%$^2$) Second affiliation
\end{center}

%\hspace{10pt}
\vspace{0.5cm}
\normalsize

\acfp{VSM} for textual data lead to success in many \ac{NLP} tasks. Recently, prediction based word embedding models like word2vec %\autocite{mikolov_efficient_2013}
gained attention. These models build upon Distributional Semantics, i.e. a word is defined by its contexts, and scale up to billions of training tokens % \autocite{mikolov_distributed_2013}
resulting in robust embeddings for individual words. 
\acfp{CDSM} intend to create vector representations for sequences of tokens by composing word embeddings in a meaningful manner. However, it is up to debate which composition functions perform well for semantic tasks. \acp{RNN} produce appropriate results for short to medium length textual input on several semantic tasks \todo{CITATIONS}, but still suffer to handle long range dependencies and do not scale very well with text size\todo{CITATIONS}. 
\acp{RecNN} generalize \acp{RNN} by allowing arbitrary trees instead of sequences as input structure. in this means, the mean ...

In this work, we study the impact of structural complexity on semantic aware composition of linguistic tokens. Especially, we investigate if 

 generalizing  

models: tfidf, gru, sum, tree (gru + sum)
further inventions: following links (just for model==tree)
 

order aware processing to token embedding composition at sentence level by implementing (1) an averaging model and (2) a \ac{LSTM} based approach. Furthermore, we analyze the relation of order aware composition to syntactical information. We evaluate our models at the SICK relatedness prediction task.% \autocite{marelli_sick_2014}.

Our results underpin the thesis, that order aware processing is useful for semantic aware composition and subsumes syntactical information in most cases. However, there are instances of linguistic constructions in which syntactical information seems to be superior to order aware processing, namely in the presence of passive.

